{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PYSPARK - ASSESSMENT - 02\n",
        "\n",
        "**Module 1: Setup & SparkSession Initialization**"
      ],
      "metadata": {
        "id": "ZsadKzc1K2Ql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N54keSIiwIu5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"BotCampus PySpark Practice\") \\\n",
        ".master(\"local[*]\") \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "(\"Anjali\", \"Bangalore\", 24),\n",
        "(\"Ravi\", \"Hyderabad\", 28),\n",
        "(\"Kavya\", \"Delhi\", 22),\n",
        "(\"Meena\", \"Chennai\", 25),\n",
        "(\"Arjun\", \"Mumbai\", 30)\n",
        "]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.printSchema()\n",
        "df.show()\n",
        "\n",
        "# RDD Conversion\n",
        "rdd = df.rdd\n",
        "print(rdd.collect())\n",
        "print(rdd.map(lambda x: (x.name, x.city)).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7rJ3oUH1nnc",
        "outputId": "04ea6e10-e067-491d-d445-0c589752305e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n",
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Anjali|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "| Arjun|   Mumbai| 30|\n",
            "+------+---------+---+\n",
            "\n",
            "[Row(name='Anjali', city='Bangalore', age=24), Row(name='Ravi', city='Hyderabad', age=28), Row(name='Kavya', city='Delhi', age=22), Row(name='Meena', city='Chennai', age=25), Row(name='Arjun', city='Mumbai', age=30)]\n",
            "[('Anjali', 'Bangalore'), ('Ravi', 'Hyderabad'), ('Kavya', 'Delhi'), ('Meena', 'Chennai'), ('Arjun', 'Mumbai')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 2: RDDs & Transformations**"
      ],
      "metadata": {
        "id": "-gOC87d34QW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the delivery\",\n",
        "\"Meena from Hyderabad had a late order\",\n",
        "\"Ajay from Pune liked the service\",\n",
        "\"Anjali from Delhi faced UI issues\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])\n",
        "\n",
        "# a\n",
        "words = feedback.flatMap(lambda line: line.lower().split())\n",
        "\n",
        "# b\n",
        "stop_words = {\"from\", \"the\", \"a\", \"had\", \"an\", \"and\"}\n",
        "filtered_words = words.filter(lambda word: word not in stop_words)\n",
        "\n",
        "# c\n",
        "word_count = filtered_words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
        "\n",
        "# d\n",
        "top3 = word_count.takeOrdered(3, key=lambda x: -x[1])\n",
        "print(top3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL5FWCvt3u28",
        "outputId": "9218e400-4bc3-4135-a91e-bd892b1592ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('loved', 1), ('liked', 1), ('service', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 3: DataFrames & Transformation (With Joins)**"
      ],
      "metadata": {
        "id": "i7VLj_1S5qY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students = [\n",
        "(\"Amit\", \"10-A\", 89),\n",
        "(\"Kavya\", \"10-B\", 92),\n",
        "(\"Anjali\", \"10-A\", 78),\n",
        "(\"Rohit\", \"10-B\", 85),\n",
        "(\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "\n",
        "columns = [\"name\", \"section\", \"marks\"]\n",
        "\n",
        "attendance = [\n",
        "(\"Amit\", 24),\n",
        "(\"Kavya\", 22),\n",
        "(\"Anjali\", 20),\n",
        "(\"Rohit\", 25),\n",
        "(\"Sneha\", 19)\n",
        "]\n",
        "\n",
        "columns2 = [\"name\", \"days_present\"]\n",
        "\n",
        "studDF = spark.createDataFrame(students, columns)\n",
        "attDF = spark.createDataFrame(attendance, columns2)\n",
        "\n",
        "# a\n",
        "joinedDF = studDF.join(attDF, on=\"name\" )\n",
        "print(\"Joined data:\\n\")\n",
        "joinedDF.show()\n",
        "\n",
        "# b\n",
        "newDF =  joinedDF.withColumn(\"attendance_rate\", col('days_present') / 25)\n",
        "print(\"Added Attendance Rate column:\\n\")\n",
        "newDF.show()\n",
        "\n",
        "# c\n",
        "newDF = newDF.withColumn(\"grade\",\n",
        "                         when( col('marks') > 90, \"A\")\n",
        "                         .when((col('marks') <= 90) & (col('marks') >= 80), \"B\")\n",
        "                         .otherwise(\"C\")\n",
        "                         )\n",
        "print(\"Added Grade column:\\n\")\n",
        "newDF.show()\n",
        "\n",
        "# d\n",
        "filteredDF = newDF.filter(\n",
        "    (col(\"grade\").isin(\"A\", \"B\")) & (col(\"attendance_rate\") < 0.8)\n",
        "    )\n",
        "print(\"\\nFiltered Data:\\n\")\n",
        "filteredDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiBFekJg4HiN",
        "outputId": "c56bff99-16d1-4408-efd0-54ae86594a9f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joined data:\n",
            "\n",
            "+------+-------+-----+------------+\n",
            "|  name|section|marks|days_present|\n",
            "+------+-------+-----+------------+\n",
            "|  Amit|   10-A|   89|          24|\n",
            "|Anjali|   10-A|   78|          20|\n",
            "| Kavya|   10-B|   92|          22|\n",
            "| Rohit|   10-B|   85|          25|\n",
            "| Sneha|   10-C|   80|          19|\n",
            "+------+-------+-----+------------+\n",
            "\n",
            "Added Attendance Rate column:\n",
            "\n",
            "+------+-------+-----+------------+---------------+\n",
            "|  name|section|marks|days_present|attendance_rate|\n",
            "+------+-------+-----+------------+---------------+\n",
            "|  Amit|   10-A|   89|          24|           0.96|\n",
            "|Anjali|   10-A|   78|          20|            0.8|\n",
            "| Kavya|   10-B|   92|          22|           0.88|\n",
            "| Rohit|   10-B|   85|          25|            1.0|\n",
            "| Sneha|   10-C|   80|          19|           0.76|\n",
            "+------+-------+-----+------------+---------------+\n",
            "\n",
            "Added Grade column:\n",
            "\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  name|section|marks|days_present|attendance_rate|grade|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  Amit|   10-A|   89|          24|           0.96|    B|\n",
            "|Anjali|   10-A|   78|          20|            0.8|    C|\n",
            "| Kavya|   10-B|   92|          22|           0.88|    A|\n",
            "| Rohit|   10-B|   85|          25|            1.0|    B|\n",
            "| Sneha|   10-C|   80|          19|           0.76|    B|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "\n",
            "\n",
            "Filtered Data:\n",
            "\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "| name|section|marks|days_present|attendance_rate|grade|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "|Sneha|   10-C|   80|          19|           0.76|    B|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 4: Ingest CSV & JSON, Save to Parquet**"
      ],
      "metadata": {
        "id": "VldQW2-S9PUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFk8RKBS9Sqz",
        "outputId": "655441c8-384c-4848-af31-b08b73b400d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csvPath = (\n",
        "    \"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "    \"Aug05-Day12/Assessment02/4/employee.csv\"\n",
        ")\n",
        "\n",
        "jsonPath = (\n",
        "    \"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "    \"Aug05-Day12/Assessment02/4/employee.json\"\n",
        ")\n",
        "\n",
        "# a\n",
        "emp1DF = spark.read.csv(csvPath, header=True, inferSchema=True)\n",
        "emp2DF = spark.read.option(\"multiline\",\"true\").json(jsonPath)\n",
        "\n",
        "print(\"Employee CSV schema:\\n\")\n",
        "emp1DF.printSchema()\n",
        "emp1DF.show()\n",
        "\n",
        "print(\"Employee JSON schema:\\n\")\n",
        "emp2DF.printSchema()\n",
        "emp2DF.show()\n",
        "\n",
        "# b\n",
        "flattend_emp = emp2DF.select(\n",
        "    col(\"id\"),\n",
        "    col(\"name\"),\n",
        "    col(\"contact.email\").alias(\"email\"),\n",
        "    col(\"contact.city\").alias(\"city\"),\n",
        "    explode(col(\"skills\")).alias(\"skill\")\n",
        ")\n",
        "print(\"Flattened Employee JSON:\\n\")\n",
        "flattend_emp.show()\n",
        "\n",
        "# c\n",
        "csvOutPath = (\n",
        "    \"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "    \"Aug05-Day12/Assessment02/4/output/employee1.parquet\"\n",
        ")\n",
        "\n",
        "jsonOutPath = (\n",
        "    \"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "    \"Aug05-Day12/Assessment02/4/0utput/employee2.parquet\"\n",
        ")\n",
        "\n",
        "emp1DF.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"csvOutPath\")\n",
        "flattend_emp.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"jsonOutOath\")\n",
        "print(\"Files Saved Successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_18A_nMW-chb",
        "outputId": "c100d7f7-72ba-44e0-e7cb-b1e1a7d3dde1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee CSV schema:\n",
            "\n",
            "root\n",
            " |-- emp_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+------+-----+-------+---------+------+\n",
            "|emp_id| name|   dept|     city|salary|\n",
            "+------+-----+-------+---------+------+\n",
            "|   101| Anil|     IT|Bangalore| 80000|\n",
            "|   102|Kiran|     HR|   Mumbai| 65000|\n",
            "|   103|Deepa|Finance|  Chennai| 72000|\n",
            "+------+-----+-------+---------+------+\n",
            "\n",
            "Employee JSON schema:\n",
            "\n",
            "root\n",
            " |-- contact: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- email: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+--------------------+---+-------+--------------------+\n",
            "|             contact| id|   name|              skills|\n",
            "+--------------------+---+-------+--------------------+\n",
            "|{Hyderabad, nandi...|201|Nandini|[Python, Spark, SQL]|\n",
            "+--------------------+---+-------+--------------------+\n",
            "\n",
            "Flattened Employee JSON:\n",
            "\n",
            "+---+-------+-----------------+---------+------+\n",
            "| id|   name|            email|     city| skill|\n",
            "+---+-------+-----------------+---------+------+\n",
            "|201|Nandini|nandi@example.com|Hyderabad|Python|\n",
            "|201|Nandini|nandi@example.com|Hyderabad| Spark|\n",
            "|201|Nandini|nandi@example.com|Hyderabad|   SQL|\n",
            "+---+-------+-----------------+---------+------+\n",
            "\n",
            "Files Saved Successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 5: Spark SQL with Temp Views**"
      ],
      "metadata": {
        "id": "2uhnQjCwB2cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newDF.createOrReplaceTempView(\"students_view\")\n",
        "print(\"Student View:\\n\")\n",
        "spark.sql(\"SELECT * FROM students_view\").show()\n",
        "\n",
        "# a\n",
        "print(\"Average marks per section:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT section, AVG(marks) AS avg_marks\n",
        "FROM students_view\n",
        "GROUP BY section\n",
        "ORDER BY section\n",
        "\"\"\").show()\n",
        "\n",
        "# b\n",
        "print(\"Top Scorers:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT section, name, marks\n",
        "FROM students_view s1\n",
        "WHERE marks = (\n",
        "  SELECT MAX(marks)\n",
        "  FROM students_view s2\n",
        "  WHERE s1.section = s2.section\n",
        ")\n",
        "\"\"\").show()\n",
        "\n",
        "# c\n",
        "print(\"Grade wise student count:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT grade, COUNT(name) AS student_count\n",
        "FROM students_view\n",
        "GROUP BY grade\n",
        "ORDER BY grade\n",
        "\"\"\").show()\n",
        "\n",
        "# d\n",
        "print(\"Students with marks above average:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT * FROM students_view\n",
        "WHERE marks > (SELECT AVG(marks) FROM students_view)\n",
        "\"\"\").show()\n",
        "\n",
        "# e\n",
        "print(\"Attendance-adjusted performance:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "select name, section, marks, attendance_rate,\n",
        "round(marks * attendance_rate, 2) as adjusted_score\n",
        "from students_view\n",
        "order by adjusted_score desc\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXc8vf4wB6nD",
        "outputId": "f4593156-93bf-40b8-f735-0f7fb5bfcee1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student View:\n",
            "\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  name|section|marks|days_present|attendance_rate|grade|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  Amit|   10-A|   89|          24|           0.96|    B|\n",
            "|Anjali|   10-A|   78|          20|            0.8|    C|\n",
            "| Kavya|   10-B|   92|          22|           0.88|    A|\n",
            "| Rohit|   10-B|   85|          25|            1.0|    B|\n",
            "| Sneha|   10-C|   80|          19|           0.76|    B|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "\n",
            "Average marks per section:\n",
            "\n",
            "+-------+---------+\n",
            "|section|avg_marks|\n",
            "+-------+---------+\n",
            "|   10-A|     83.5|\n",
            "|   10-B|     88.5|\n",
            "|   10-C|     80.0|\n",
            "+-------+---------+\n",
            "\n",
            "Top Scorers:\n",
            "\n",
            "+-------+-----+-----+\n",
            "|section| name|marks|\n",
            "+-------+-----+-----+\n",
            "|   10-A| Amit|   89|\n",
            "|   10-B|Kavya|   92|\n",
            "|   10-C|Sneha|   80|\n",
            "+-------+-----+-----+\n",
            "\n",
            "Grade wise student count:\n",
            "\n",
            "+-----+-------------+\n",
            "|grade|student_count|\n",
            "+-----+-------------+\n",
            "|    A|            1|\n",
            "|    B|            3|\n",
            "|    C|            1|\n",
            "+-----+-------------+\n",
            "\n",
            "Students with marks above average:\n",
            "\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "| name|section|marks|days_present|attendance_rate|grade|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "| Amit|   10-A|   89|          24|           0.96|    B|\n",
            "|Kavya|   10-B|   92|          22|           0.88|    A|\n",
            "|Rohit|   10-B|   85|          25|            1.0|    B|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "\n",
            "Attendance-adjusted performance:\n",
            "\n",
            "+------+-------+-----+---------------+--------------+\n",
            "|  name|section|marks|attendance_rate|adjusted_score|\n",
            "+------+-------+-----+---------------+--------------+\n",
            "|  Amit|   10-A|   89|           0.96|         85.44|\n",
            "| Rohit|   10-B|   85|            1.0|          85.0|\n",
            "| Kavya|   10-B|   92|           0.88|         80.96|\n",
            "|Anjali|   10-A|   78|            0.8|          62.4|\n",
            "| Sneha|   10-C|   80|           0.76|          60.8|\n",
            "+------+-------+-----+---------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 6: Partitioned Data & Incremental Loading**"
      ],
      "metadata": {
        "id": "ogAw2apRG06O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial load\n",
        "outPath = \"output/students/\"\n",
        "newDF.write.partitionBy(\"section\").parquet(\"outPath\")\n",
        "\n",
        "# Incremental Load\n",
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(outPath)\n",
        "\n",
        "df_inc.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKvTnU7xG4N-",
        "outputId": "c73bad5d-3aeb-42cd-c84c-3d737af27a5e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- section: string (nullable = true)\n",
            " |-- marks: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# a\n",
        "print(\"Folders List:\\n\")\n",
        "print(os.listdir(outPath))\n",
        "\n",
        "# b\n",
        "df_10A = spark.read.parquet(\"output/students/section=10-A\")\n",
        "df_10A.show()\n",
        "\n",
        "# c\n",
        "count_after = df_10A.count()\n",
        "print(f\"Number of students in section 10-A after incremental load: {count_after}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJTn-ogLHi3t",
        "outputId": "053267b9-da18-4587-d0db-a384041848d1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders List:\n",
            "\n",
            "['._SUCCESS.crc', 'section=10-A', '_SUCCESS']\n",
            "+-----+-----+\n",
            "| name|marks|\n",
            "+-----+-----+\n",
            "|Tejas|   91|\n",
            "+-----+-----+\n",
            "\n",
            "Number of students in section 10-A after incremental load: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 7: ETL Pipeline â€“ End to End**"
      ],
      "metadata": {
        "id": "vXrrhSQDJCGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a\n",
        "empPath = (\"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "          \"Aug05-Day12/Assessment02/7/employee.csv\"\n",
        "          )\n",
        "df_emp = spark.read.csv(empPath, header = True, inferSchema= True)\n",
        "df_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id4t3tdbJFhv",
        "outputId": "8977cec2-8c42-4955-bbd7-6c56e6133ebb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+\n",
            "|emp_id|  name|   dept|salary|bonus|\n",
            "+------+------+-------+------+-----+\n",
            "|     1| Arjun|     IT| 75000| 5000|\n",
            "|     2| Kavya|     HR| 62000| NULL|\n",
            "|     3| Sneha|Finance| 68000| 4000|\n",
            "|     4|Ramesh|  Sales| 58000| NULL|\n",
            "+------+------+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b\n",
        "df_emp = df_emp.fillna({\"bonus\": 2000})\n",
        "df_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNFWBDmtJzsl",
        "outputId": "5e27e96f-da2d-4bce-f5b8-d20e9e6e5319"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+\n",
            "|emp_id|  name|   dept|salary|bonus|\n",
            "+------+------+-------+------+-----+\n",
            "|     1| Arjun|     IT| 75000| 5000|\n",
            "|     2| Kavya|     HR| 62000| 2000|\n",
            "|     3| Sneha|Finance| 68000| 4000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|\n",
            "+------+------+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c\n",
        "df_emp = df_emp.withColumn(\"total_ctc\", expr(\"salary + bonus\"))\n",
        "df_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_RUbh88J35k",
        "outputId": "a936bcfa-2d39-48b9-fa8b-7e38db6bafc4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+---------+\n",
            "|emp_id|  name|   dept|salary|bonus|total_ctc|\n",
            "+------+------+-------+------+-----+---------+\n",
            "|     1| Arjun|     IT| 75000| 5000|    80000|\n",
            "|     2| Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3| Sneha|Finance| 68000| 4000|    72000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|    60000|\n",
            "+------+------+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d\n",
        "df_filtered = df_emp.filter(\"total_ctc > 65000\")\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL2tQa9vJ9JV",
        "outputId": "847e7d40-381a-4925-890e-01d6754eea67"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 75000| 5000|    80000|\n",
            "|     3|Sneha|Finance| 68000| 4000|    72000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e\n",
        "empOutPath1 = (\"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "              \"Aug05-Day12/Assessment02/Output1/employee.parquet\"\n",
        "              )\n",
        "empOutPath2 = (\"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "              \"Aug05-Day12/Assessment02/Output2/employee.json\"\n",
        "              )\n",
        "\n",
        "df_emp.write.mode(\"overwrite\").parquet(empOutPath1)\n",
        "df_emp.write.mode(\"overwrite\").json(empOutPath2)\n",
        "print(\"Files saved successsfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valUf60AJ_48",
        "outputId": "499c854e-672b-48cf-a3bf-781cf5212443"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files saved successsfully\n"
          ]
        }
      ]
    }
  ]
}