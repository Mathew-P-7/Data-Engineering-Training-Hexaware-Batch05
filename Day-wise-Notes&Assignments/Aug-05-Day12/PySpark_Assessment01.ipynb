{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PYSPARK - ASSESSMENT - O1\n",
        "\n",
        "**1. PySpark Setup & Initialization**"
      ],
      "metadata": {
        "id": "9aUkaFX6vqLV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GW1UptV2vSub"
      },
      "outputs": [],
      "source": [
        "# 1.1\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"BotCampus Intermediate Session\") \\\n",
        ".master(\"local[*]\") \\\n",
        ".getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2\n",
        "data = [(\"Ananya\", \"Bangalore\", 24),\n",
        "        (\"Ravi\", \"Hyderabad\", 28),\n",
        "        (\"Kavya\", \"Delhi\", 22),\n",
        "        (\"Meena\", \"Chennai\", 25)]\n",
        "\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvogvgr0wDHl",
        "outputId": "0adc2023-51e4-4a20-a265-9ecd3c5cdb06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. RDDs & Transformations**"
      ],
      "metadata": {
        "id": "lPt_pKqwwfuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import linesep\n",
        "# 2.1\n",
        "feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the mobile app\",\n",
        "\"Meena from Delhi reported poor response time\",\n",
        "\"Ajay from Pune liked the delivery speed\",\n",
        "\"Ananya from Hyderabad had an issue with UI\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])\n",
        "\n",
        "# 2.1.1\n",
        "words=feedback.flatMap(lambda line:line.lower().split())\n",
        "total_words=words.count()\n",
        "print(\"Total word count:\",total_words)\n",
        "\n",
        "# 2.1.2\n",
        "word_counts=words.map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b)\n",
        "top_3=word_counts.takeOrdered(3,key=lambda x:-x[1])\n",
        "print(\"\\nTop 3 common words:\\n\",top_3)\n",
        "\n",
        "# 2.1.3\n",
        "stopwords = {\"from\", \"with\", \"the\", \"and\", \"an\", \"had\", \"was\", \"of\"}\n",
        "filtered_words=words.filter(lambda word:word.lower() not in stopwords)\n",
        "filtered_counts=filtered_words.map(lambda word:(word,1)) \\\n",
        "              .reduceByKey(lambda a,b:a+b)\n",
        "print(\"\\nStop Words removed successfully\")\n",
        "\n",
        "# 2.1.4\n",
        "word_count_dict=dict(filtered_counts.collect())\n",
        "print(\"\\nWord-Count Dictionary:\\n\",word_count_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhjeiuzKwlxx",
        "outputId": "49c6f380-37d7-400b-836a-66331722bae8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count: 35\n",
            "\n",
            "Top 3 common words:\n",
            " [('from', 5), ('the', 2), ('loved', 1)]\n",
            "\n",
            "Stop Words removed successfully\n",
            "\n",
            "Word-Count Dictionary:\n",
            " {'loved': 1, 'app': 1, 'poor': 1, 'response': 1, 'liked': 1, 'speed': 1, 'ananya': 1, 'issue': 1, 'rohit': 1, 'mumbai': 1, 'positive': 1, 'feedback': 1, 'ravi': 1, 'bangalore': 1, 'mobile': 1, 'meena': 1, 'delhi': 1, 'reported': 1, 'time': 1, 'ajay': 1, 'pune': 1, 'delivery': 1, 'hyderabad': 1, 'ui': 1, 'gave': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. DataFrames - Transformations**\n"
      ],
      "metadata": {
        "id": "zYJGJw4K3n0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "scores = [\n",
        "(\"Ravi\", \"Math\", 88),\n",
        "(\"Ananya\", \"Science\", 92),\n",
        "(\"Kavya\", \"English\", 79),\n",
        "(\"Ravi\", \"English\", 67),\n",
        "(\"Neha\", \"Math\", 94),\n",
        "(\"Meena\", \"Science\", 85)\n",
        "]\n",
        "\n",
        "columns = [\"name\",\"subject\",\"score\"]\n",
        "df_scores =  spark.createDataFrame(scores, columns)\n",
        "\n",
        "# 3.1.1\n",
        "df_scores = df_scores.withColumn('grade', when(col('score') >= 90, \"A\")\\\n",
        "                                 .when(col('score') >= 80, \"B\")\\\n",
        "                                 .when(col('score') >= 70,\"C\")\\\n",
        "                                 .otherwise(\"D\"))\n",
        "print(\"Added new Grade column:\\n\")\n",
        "df_scores.show()\n",
        "\n",
        "# 3.1.2\n",
        "print(\"Average Subject Scores:\\n\")\n",
        "df_scores.groupBy('subject').agg(avg('score').alias(\"average_score\")).show()\n",
        "\n",
        "# 3.1.3\n",
        "df_scores = df_scores.withColumn('difficulty',\n",
        "                    when(col('subject').isin('Math','Science'), \"Difficult\")\\\n",
        "                    .otherwise(\"Easy\"))\n",
        "print(\"Added new Difficulty column:\\n\")\n",
        "df_scores.show()\n",
        "\n",
        "# 3.1.4\n",
        "window = Window.partitionBy('subject').orderBy(col('score').desc())\n",
        "df_scores = df_scores.withColumn('rank', rank().over(window))\n",
        "\n",
        "print(\"Students Rank:\\n\")\n",
        "df_scores.show()\n",
        "\n",
        "# 3.1.5\n",
        "# My user derfined funtion: Upper Case conversion\n",
        "def to_upper(word):\n",
        "  if word is not None:\n",
        "    return word.upper()\n",
        "  else: return None\n",
        "\n",
        "uppercase_udf = udf(to_upper, StringType())\n",
        "\n",
        "df_scores = df_scores.withColumn('name', uppercase_udf(col('name')))\n",
        "print(\"Uppercase Name:\\n\")\n",
        "df_scores.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Rv2vfc3tBU",
        "outputId": "27cf3716-6e4c-45f6-82ac-26ebfc1cd288"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added new Grade column:\n",
            "\n",
            "+------+-------+-----+-----+\n",
            "|  name|subject|score|grade|\n",
            "+------+-------+-----+-----+\n",
            "|  Ravi|   Math|   88|    B|\n",
            "|Ananya|Science|   92|    A|\n",
            "| Kavya|English|   79|    C|\n",
            "|  Ravi|English|   67|    D|\n",
            "|  Neha|   Math|   94|    A|\n",
            "| Meena|Science|   85|    B|\n",
            "+------+-------+-----+-----+\n",
            "\n",
            "Average Subject Scores:\n",
            "\n",
            "+-------+-------------+\n",
            "|subject|average_score|\n",
            "+-------+-------------+\n",
            "|Science|         88.5|\n",
            "|   Math|         91.0|\n",
            "|English|         73.0|\n",
            "+-------+-------------+\n",
            "\n",
            "Added new Difficulty column:\n",
            "\n",
            "+------+-------+-----+-----+----------+\n",
            "|  name|subject|score|grade|difficulty|\n",
            "+------+-------+-----+-----+----------+\n",
            "|  Ravi|   Math|   88|    B| Difficult|\n",
            "|Ananya|Science|   92|    A| Difficult|\n",
            "| Kavya|English|   79|    C|      Easy|\n",
            "|  Ravi|English|   67|    D|      Easy|\n",
            "|  Neha|   Math|   94|    A| Difficult|\n",
            "| Meena|Science|   85|    B| Difficult|\n",
            "+------+-------+-----+-----+----------+\n",
            "\n",
            "Students Rank:\n",
            "\n",
            "+------+-------+-----+-----+----------+----+\n",
            "|  name|subject|score|grade|difficulty|rank|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "| Kavya|English|   79|    C|      Easy|   1|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|\n",
            "|Ananya|Science|   92|    A| Difficult|   1|\n",
            "| Meena|Science|   85|    B| Difficult|   2|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "\n",
            "Uppercase Name:\n",
            "\n",
            "+------+-------+-----+-----+----------+----+\n",
            "|  name|subject|score|grade|difficulty|rank|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "| KAVYA|English|   79|    C|      Easy|   1|\n",
            "|  RAVI|English|   67|    D|      Easy|   2|\n",
            "|  NEHA|   Math|   94|    A| Difficult|   1|\n",
            "|  RAVI|   Math|   88|    B| Difficult|   2|\n",
            "|ANANYA|Science|   92|    A| Difficult|   1|\n",
            "| MEENA|Science|   85|    B| Difficult|   2|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Ingest CSV & JSON - Save to Parquet**"
      ],
      "metadata": {
        "id": "2SJ5nxr8BC7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ast8kXY7mDU",
        "outputId": "d2a6b0c2-3310-4494-b33b-12fbd62b6474"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "studPath = \"/content/drive/MyDrive/Hexware_Training_DataEngineering/Aug05-Day12/Assessment01/students.csv\"\n",
        "empPath = \"/content/drive/MyDrive/Hexware_Training_DataEngineering/Aug05-Day12/Assessment01/employee_nested.json\"\n",
        "\n",
        "studDF = spark.read.csv(studPath, header = True, inferSchema = True)\n",
        "empDF = spark.read.option(\"multiline\", \"true\").json(empPath)\n",
        "\n",
        "print(\"Student Schema:\\n\")\n",
        "studDF.printSchema()\n",
        "studDF.show()\n",
        "\n",
        "print(\"Employee Schema:\\n\")\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YZxZBt5C3lC",
        "outputId": "dd6c60a9-d460-4239-9642-b609d835165c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Schema:\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+---+-----+----------+---------+------+\n",
            "| id| name|department|     city|salary|\n",
            "+---+-----+----------+---------+------+\n",
            "|  1| Amit|        IT|Bangalore| 78000|\n",
            "|  2|Kavya|        HR|  Chennai| 62000|\n",
            "|  3|Arjun|   Finance|Hyderabad| 55000|\n",
            "+---+-----+----------+---------+------+\n",
            "\n",
            "Employee Schema:\n",
            "\n",
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- pincode: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------+---+-----+---------------+\n",
            "|address         |id |name |skills         |\n",
            "+----------------+---+-----+---------------+\n",
            "|{Mumbai, 400001}|101|Sneha|[Python, Spark]|\n",
            "+----------------+---+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.3\n",
        "flattened_emp = empDF.select(\n",
        "    col('id'),\n",
        "    col('name'),\n",
        "    col('address.city').alias('city'),\n",
        "    col('address.pincode').alias('pincode'),\n",
        "    explode(col('skills')).alias('skill')\n",
        ")\n",
        "\n",
        "print(\"Flattened Employee Json:\\n\")\n",
        "flattened_emp.printSchema()\n",
        "flattened_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU5WFz5-Gu6A",
        "outputId": "7c1f0ff8-6468-426c-abf3-b21446730cfc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened Employee Json:\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- pincode: long (nullable = true)\n",
            " |-- skill: string (nullable = true)\n",
            "\n",
            "+---+-----+------+-------+------+\n",
            "| id| name|  city|pincode| skill|\n",
            "+---+-----+------+-------+------+\n",
            "|101|Sneha|Mumbai| 400001|Python|\n",
            "|101|Sneha|Mumbai| 400001| Spark|\n",
            "+---+-----+------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.4\n",
        "studOutPath = (\"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "              \"Aug05-Day12/Assessment01/Output/students_parqueet.csv\"\n",
        "              )\n",
        "empOutPath = (\"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "              \"Aug05-Day12/Assessment01/Output/employees_parqueet.csv\"\n",
        "              )\n",
        "\n",
        "studDF.coalesce(1).write.mode(\"overwrite\").parquet(studOutPath)\n",
        "flattened_emp.coalesce(1).write.mode(\"overwrite\").parquet(empOutPath)\n",
        "print('Files Saved successfully')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2TFG1mBIg4l",
        "outputId": "d222af7a-e900-4ef3-df88-a768f324e547"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files Saved successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. SPARK SQL - Temp Views & Queries**"
      ],
      "metadata": {
        "id": "GtsjxsY6ZzRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1\n",
        "# a\n",
        "df_scores.createOrReplaceTempView(\"exam_scores\")\n",
        "\n",
        "print(\"Exam Scores:\\n\")\n",
        "spark.sql(\"SELECT * FROM exam_scores\").show()\n",
        "\n",
        "print(\"\\nTop Scorers per subject:\\n\")\n",
        "spark.sql(\"\"\"SELECT * From exam_scores WHERE rank = 1\"\"\").show()\n",
        "\n",
        "# b\n",
        "print(\"\\nStudent Count per Grade:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT grade, COUNT(name) AS student_count\n",
        "FROM exam_scores\n",
        "GROUP BY grade\n",
        "ORDER BY grade\"\"\").show()\n",
        "\n",
        "# c\n",
        "print(\"\\nStudents with more than one subject:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT name, Count(subject) AS subject_count\n",
        "FROM exam_scores\n",
        "GROUP BY name\n",
        "HAVING subject_count > 1\n",
        "\"\"\").show()\n",
        "\n",
        "\n",
        "# d\n",
        "print(\"\\nAverage Score per Subject:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT subject, AVG(score) AS average_score\n",
        "FROM exam_scores\n",
        "GROUP BY subject\n",
        "HAVING average_score > 85\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq1aV0dAZ8pJ",
        "outputId": "16d1a2a5-0b1a-4554-bd37-4f5f9e5bf99f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exam Scores:\n",
            "\n",
            "+------+-------+-----+-----+----------+----+\n",
            "|  name|subject|score|grade|difficulty|rank|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "| KAVYA|English|   79|    C|      Easy|   1|\n",
            "|  RAVI|English|   67|    D|      Easy|   2|\n",
            "|  NEHA|   Math|   94|    A| Difficult|   1|\n",
            "|  RAVI|   Math|   88|    B| Difficult|   2|\n",
            "|ANANYA|Science|   92|    A| Difficult|   1|\n",
            "| MEENA|Science|   85|    B| Difficult|   2|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "\n",
            "\n",
            "Top Scorers per subject:\n",
            "\n",
            "+------+-------+-----+-----+----------+----+\n",
            "|  name|subject|score|grade|difficulty|rank|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "| KAVYA|English|   79|    C|      Easy|   1|\n",
            "|  NEHA|   Math|   94|    A| Difficult|   1|\n",
            "|ANANYA|Science|   92|    A| Difficult|   1|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "\n",
            "\n",
            "Student Count per Grade:\n",
            "\n",
            "+-----+-------------+\n",
            "|grade|student_count|\n",
            "+-----+-------------+\n",
            "|    A|            2|\n",
            "|    B|            2|\n",
            "|    C|            1|\n",
            "|    D|            1|\n",
            "+-----+-------------+\n",
            "\n",
            "\n",
            "Students with more than one subject:\n",
            "\n",
            "+----+-------------+\n",
            "|name|subject_count|\n",
            "+----+-------------+\n",
            "|RAVI|            2|\n",
            "+----+-------------+\n",
            "\n",
            "\n",
            "Average Score per Subject:\n",
            "\n",
            "+-------+-------------+\n",
            "|subject|average_score|\n",
            "+-------+-------------+\n",
            "|Science|         88.5|\n",
            "|   Math|         91.0|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.2\n",
        "attendance = [\n",
        "    (\"KAVYA\", 21),\n",
        "    (\"RAVI\", 22),\n",
        "    (\"NEHA\", 25),\n",
        "    (\"ANANYA\", 18),\n",
        "    (\"MEENA\", 15)\n",
        "]\n",
        "\n",
        "columns = [\"name\",\"days_present\"]\n",
        "df_attendance = spark.createDataFrame(attendance, columns)\n",
        "\n",
        "df_attendance.createOrReplaceTempView(\"attendance\")\n",
        "\n",
        "# a\n",
        "print(\"Joined data:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT e.*, a.days_present\n",
        "FROM exam_scores e\n",
        "LEFT JOIN attendance a\n",
        "ON e.name = a.name\n",
        "\"\"\").show()\n",
        "\n",
        "# b\n",
        "print(\"\\nUpdated Grades:\\n\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT e.name, e.subject, e.score, a.days_present, e.grade,\n",
        "  CASE\n",
        "    WHEN a.days_present < 20 AND e.grade = 'A' THEN 'B'\n",
        "    WHEN a.days_present < 20 AND e.grade = 'B' THEN 'C'\n",
        "    WHEN a.days_present < 20 AND e.grade = 'C' THEN 'D'\n",
        "    WHEN a.days_present < 20 AND e.grade = 'D' THEN 'E'\n",
        "    ELSE e.grade\n",
        "  END AS updated_grade\n",
        "FROM exam_scores e\n",
        "LEFT JOIN attendance a\n",
        "ON e.name = a.name\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAW6YNf5gJrq",
        "outputId": "b4c9360d-4ea0-4d49-a118-76a4e5b9f524"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joined data:\n",
            "\n",
            "+------+-------+-----+-----+----------+----+------------+\n",
            "|  name|subject|score|grade|difficulty|rank|days_present|\n",
            "+------+-------+-----+-----+----------+----+------------+\n",
            "|  NEHA|   Math|   94|    A| Difficult|   1|          25|\n",
            "| MEENA|Science|   85|    B| Difficult|   2|          15|\n",
            "|  RAVI|English|   67|    D|      Easy|   2|          22|\n",
            "|  RAVI|   Math|   88|    B| Difficult|   2|          22|\n",
            "|ANANYA|Science|   92|    A| Difficult|   1|          18|\n",
            "| KAVYA|English|   79|    C|      Easy|   1|          21|\n",
            "+------+-------+-----+-----+----------+----+------------+\n",
            "\n",
            "\n",
            "Updated Grades:\n",
            "\n",
            "+------+-------+-----+------------+-----+-------------+\n",
            "|  name|subject|score|days_present|grade|updated_grade|\n",
            "+------+-------+-----+------------+-----+-------------+\n",
            "|  RAVI|   Math|   88|          22|    B|            B|\n",
            "|ANANYA|Science|   92|          18|    A|            B|\n",
            "| KAVYA|English|   79|          21|    C|            C|\n",
            "|  NEHA|   Math|   94|          25|    A|            A|\n",
            "| MEENA|Science|   85|          15|    B|            C|\n",
            "|  RAVI|English|   67|          22|    D|            D|\n",
            "+------+-------+-----+------------+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Partitioned Load (Full + Incremental)**"
      ],
      "metadata": {
        "id": "CNqjq_RDlIh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial load\n",
        "outPath = \"/tmp/scores/\"\n",
        "df_scores.write.partitionBy(\"subject\").mode(\"overwrite\").parquet(outPath)\n",
        "\n",
        "# Incremental load\n",
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_inc = spark.createDataFrame(incremental, columns)\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")\n",
        "\n",
        "df_inc.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFGSauiklNyw",
        "outputId": "d0b3a33a-f675-46ea-e21a-c12a1a7c49e0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- subject: string (nullable = true)\n",
            " |-- score: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a\n",
        "import os\n",
        "print(\"Folders in /tmp/scores/:\\n\")\n",
        "print(os.listdir(\"/tmp/scores/\"))\n",
        "\n",
        "# b\n",
        "math_df = spark.read.parquet(\"/tmp/scores/subject=Math\")\n",
        "print(\"\\nMath Scores:\\n\")\n",
        "math_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1xyDuGaoCk-",
        "outputId": "97b25fb6-b0e3-4dbc-8a49-90b2b66a8b5d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders in /tmp/scores/:\n",
            "\n",
            "['._SUCCESS.crc', 'subject=Science', 'subject=English', '_SUCCESS', 'subject=Math']\n",
            "\n",
            "Math Scores:\n",
            "\n",
            "+-----+-----+-----+----------+----+\n",
            "| name|score|grade|difficulty|rank|\n",
            "+-----+-----+-----+----------+----+\n",
            "| NEHA|   94|    A| Difficult|   1|\n",
            "| RAVI|   88|    B| Difficult|   2|\n",
            "|Meena|   93| NULL|      NULL|NULL|\n",
            "+-----+-----+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. ETL: Clean, Transform, Load**"
      ],
      "metadata": {
        "id": "JIIZXeNXoyjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a\n",
        "empPath = (\"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "          \"Aug05-Day12/Assessment01/employee.csv\"\n",
        "          )\n",
        "df_employee = spark.read.csv(empPath, header = True, inferSchema= True)\n",
        "df_employee.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVDDbApopB-i",
        "outputId": "fe9ffda7-f67c-4e9e-da1e-938fada03a79"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| NULL|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b\n",
        "df_employee = df_employee.fillna({\"bonus\": 2000})"
      ],
      "metadata": {
        "id": "oXm6kc7mrGh-"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c\n",
        "df_employee = df_employee.withColumn(\"total_ctc\", col('salary')+col('bonus'))\n",
        "df_employee.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4oosGqWsBWi",
        "outputId": "5dfd9f5a-1dac-4b58-e498-416ceb7ba5df"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3|Sneha|Finance| 55000| 3000|    58000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d\n",
        "df_employee.filter(col('total_ctc') > 60000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eeb4pO2Zsaf9",
        "outputId": "9f004d73-211c-49d1-9fe6-ae699bf9e132"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----+------+-----+---------+\n",
            "|emp_id| name|dept|salary|bonus|total_ctc|\n",
            "+------+-----+----+------+-----+---------+\n",
            "|     1|Arjun|  IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|  HR| 62000| 2000|    64000|\n",
            "+------+-----+----+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e\n",
        "empOutPath1 = (\"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "              \"Aug05-Day12/Assessment01/Output1/employee.parquet\"\n",
        "              )\n",
        "empOutPath2 = (\"/content/drive/MyDrive/Hexware_Training_DataEngineering/\"\n",
        "              \"Aug05-Day12/Assessment01/Output2/employee.json\"\n",
        "              )\n",
        "\n",
        "df_employee.write.mode(\"overwrite\").parquet(empOutPath1)\n",
        "df_employee.write.mode(\"overwrite\").json(empOutPath2)\n",
        "print(\"Files saved successsfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz23EZbhssXG",
        "outputId": "c21ef39a-287c-4f14-efc8-c1daa6610b26"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files saved successsfully\n"
          ]
        }
      ]
    }
  ]
}